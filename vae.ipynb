{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import (Lambda, Input, Reshape,\n",
    "                          Dense, \n",
    "                          Conv2D, Conv2DTranspose,\n",
    "                          Flatten, MaxPool2D,)\n",
    "from keras.losses import mse, mae, binary_crossentropy\n",
    "from keras.models import Model\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 1024, 1, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 512, 1, 2)    52          input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 256, 1, 4)    204         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 1024)         0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           16400       flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 4)            68          dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 4)            68          dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 4)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 16,792\n",
      "Trainable params: 16,792\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (1024,1,1) # if you want 1D fft inputs, make the input_shape (nfft,1,1)\n",
    "filters = 1\n",
    "layers = 2\n",
    "kernel_size = [5,5]\n",
    "strides = [2,2]\n",
    "dilation = [1,1]\n",
    "intermediate = 16\n",
    "latent_dim = 4\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "x = inputs\n",
    "for i in range(layers):\n",
    "    filters *= 2\n",
    "    x = Conv2D(filters,\n",
    "               kernel_size=kernel_size,\n",
    "               strides=strides,\n",
    "               dilation_rate=dilation,\n",
    "               activation='relu',\n",
    "               padding='same')(x)\n",
    "\n",
    "\n",
    "# shape info needed to build decoder model\n",
    "shape = K.int_shape(x)\n",
    "\n",
    "# generate latent vector Q(z|X)\n",
    "x = Flatten()(x)\n",
    "x = Dense(intermediate, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "encoder = Model(inputs,[z_mean, z_log_var, z],name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1024)              5120      \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 256, 1, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_15 (Conv2DT (None, 512, 2, 4)         404       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DT (None, 1024, 4, 2)        202       \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 1024, 4, 1)        51        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1024, 1, 1)        0         \n",
      "=================================================================\n",
      "Total params: 5,777\n",
      "Trainable params: 5,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs)\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "for i in range(layers):\n",
    "    x = Conv2DTranspose(filters=filters,\n",
    "                        kernel_size=kernel_size,\n",
    "                        strides=strides,\n",
    "                        activation='relu',\n",
    "                        padding='same')(x)\n",
    "    filters //= 2\n",
    "\n",
    "x = Conv2DTranspose(filters=1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    activation='relu', # we may need to play with this, the keras example has sigmoid\n",
    "                    padding='same',\n",
    "                    name='decoder_output')(x)\n",
    "\n",
    "# if 1D input we need to maxpool dim 2 to collapse the extra bins the Transpose conv2D put in that dimension\n",
    "if input_shape[1]==1:\n",
    "    out_shape = K.int_shape(x)\n",
    "    outputs = MaxPool2D(pool_size=(1,out_shape[2]), strides=None, padding='valid')(x)\n",
    "else:\n",
    "    outputs = x\n",
    "\n",
    "decoder = Model(latent_inputs,outputs)\n",
    "decoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "reconstruction_loss = binary_crossentropy(K.flatten(inputs),\n",
    "                                          K.flatten(outputs))\n",
    "\n",
    "reconstruction_loss *= input_shape[0] * input_shape[1]\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "X = np.random.uniform(0,1,(n_samples,)+input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 50s 5ms/step - loss: 709.7821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0c2c77c3c8>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 1000\n",
    "vae.fit(X,epochs=epochs,batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
